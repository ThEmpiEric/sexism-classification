{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da49d61b",
   "metadata": {},
   "source": [
    "# __Preprocesamiento__\n",
    "\n",
    "\n",
    "El siguiente Notebook contiene la etapa de procesamiento para los futuros experimentos del notebook: _model_search.ipynb_.\n",
    "\n",
    "\n",
    "__Datos__: Dataset EXIST 2024. Para la descarga de los datos consulta la sección Dataset de la página oficial de la competencia __EXIST: sEXism Identification in Social neTworks__ [Data](https://nlp.uned.es/exist2024/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "84e548b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import os \n",
    "import json \n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "148e038b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'C:\\Users\\ericl\\Maestria\\EXIST\\EXIST2021-2024_datasets\\2024 EXIST\\EXIST 2024 Tweets Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40b9df9",
   "metadata": {},
   "source": [
    "Extracción de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "e28e5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_df(file_path):\n",
    "    \"\"\" Crea una etiqueta binaria para labels_task1.  \"\"\"\n",
    "    \n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    dataset = []\n",
    "    for _, sample in data.items():           \n",
    "        labels_vector = [1 if label == \"YES\" else 0 for label in sample.get(\"labels_task1\", [])]  \n",
    "        metadata = list(sample.items())[3:-1]\n",
    "        dataset.append({\"text\" : sample.get(\"tweet\", \"\"), \n",
    "                        \"label\": labels_vector, \n",
    "                        \"split\": sample.get(\"split\",\"\"), \n",
    "                        \"metadata\": metadata})\n",
    "    return pd.DataFrame(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e3d3054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRAINING] - loaded\n",
      "[TEST] - loaded\n",
      "[DEV] - loaded\n"
     ]
    }
   ],
   "source": [
    "split_filenames = { \"training\": \"EXIST2024_training.json\", \n",
    "                    \"test\": \"EXIST2023_test_clean.json\",\n",
    "                    \"dev\": \"EXIST2024_dev.json\" }\n",
    "\n",
    "df_dict = {}\n",
    "for split, filename in split_filenames.items():\n",
    "    file_path = os.path.join(data_path, split, filename)\n",
    "    df_dict[split] = extract_df(file_path)\n",
    "    print(f\"[{split.upper()}] - loaded\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "f5466509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save raw data\n",
    "df_dict['training'].to_csv('../data/raw_train.csv', index=False, encoding='utf-8')\n",
    "df_dict['dev'].to_csv('../data/raw_val.csv', index=False, encoding='utf-8')\n",
    "df_dict['test'].to_csv('../data/raw_test.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90231929",
   "metadata": {},
   "source": [
    "Split data por lenguaje "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "71ce210b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_ds_datadict(df_dict,lang = 'ES'):\n",
    "    \"\"\"\n",
    "        Create a binary gold truth \n",
    "        Return: DatasetDict object \n",
    "    \"\"\"\n",
    "    dataset_dict = {}\n",
    "    for split, df in df_dict.items():\n",
    "        split_key = 'train' if split == 'training' else split\n",
    "        split_name = f\"{split_key.upper()}_{lang}\"\n",
    "        filt_df = df[df['split'] == split_name].copy()\n",
    "        filt_df = filt_df.drop(columns=['metadata'])\n",
    "        filt_df['label'] = filt_df['label'].apply(lambda x: 1 if sum(x) >= 1 else 0)\n",
    "        dataset_dict[split] = Dataset.from_pandas(filt_df.reset_index(drop=True))\n",
    "    return DatasetDict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "e7ec54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_ES = to_ds_datadict(df_dict,lang='ES')\n",
    "dataset_EN = to_ds_datadict(df_dict,lang='EN')    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68aeb40",
   "metadata": {},
   "source": [
    "Proporción por clase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "7833d678",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribución de etiquetas en TRAIN:\n",
      "  Etiqueta 1: 2994 (81.80%)\n",
      "  Etiqueta 0: 666 (18.20%)\n",
      "\n",
      "Distribución de etiquetas en VAL:\n",
      "  Etiqueta 1: 452 (82.33%)\n",
      "  Etiqueta 0: 97 (17.67%)\n"
     ]
    }
   ],
   "source": [
    "def class_proportions(dataset):\n",
    "    from collections import Counter\n",
    "    train_labels = dataset['training']['label']\n",
    "    train_counts = Counter(train_labels)\n",
    "    train_total = len(train_labels)\n",
    "\n",
    "    print(\"Distribución de etiquetas en TRAIN:\")\n",
    "    for label, count in train_counts.items():\n",
    "        print(f\"  Etiqueta {label}: {count} ({count/train_total:.2%})\")\n",
    "\n",
    "    val_labels = dataset['dev']['label']\n",
    "    val_counts = Counter(val_labels)\n",
    "    val_total = len(val_labels)\n",
    "\n",
    "    print(\"\\nDistribución de etiquetas en VAL:\")\n",
    "    for label, count in val_counts.items():\n",
    "        print(f\"  Etiqueta {label}: {count} ({count/val_total:.2%})\")\n",
    "\n",
    "class_proportions(dataset_ES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "bbb5a21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = dataset_ES['training']['text']\n",
    "y_train = dataset_ES['training']['label']\n",
    "X_val = dataset_ES['dev']['text']\n",
    "y_val = dataset_ES['dev']['label']\n",
    "X_test = dataset_ES['test']['text']\n",
    "y_test = dataset_ES['test']['label']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a5a44",
   "metadata": {},
   "source": [
    "Limpieza de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d732619b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "import nltk\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=True, strip_handles=False)\n",
    "spanish_stopwords = set(stopwords.words('spanish'))\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n",
    "URL_PATTERN     = re.compile(r'https?://\\S+|www\\.\\S+', flags=re.IGNORECASE)\n",
    "MENTION_PATTERN = re.compile(r'@\\w+')\n",
    "HASHTAG_PATTERN = re.compile(r'#\\w+')\n",
    "RETWEET_PATTERN = re.compile(r'\\brt\\b', flags=re.IGNORECASE)\n",
    "PUNCT_PATTERN = re.compile(r'[^\\w\\sáéíóúñüÁÉÍÓÚÑÜ0-9]')\n",
    "\n",
    "def clean_tweets(tweets, remove_stopwords: bool = True, do_stemming: bool = False, \n",
    "                 replace_mentions: bool = True) -> list[str]:\n",
    "    \n",
    "    cleaned_tweets = []\n",
    "    for raw in tweets:\n",
    "        text = raw.lower().replace('\\n', ' ')\n",
    "        text = URL_PATTERN.sub('', text)\n",
    "        # text = RETWEET_PATTERN.sub('', text)\n",
    "\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "\n",
    "        cleaned_tokens: list[str] = []\n",
    "        for t in tokens:\n",
    "            \n",
    "            if t.startswith('@'):  \n",
    "                if replace_mentions:\n",
    "                    cleaned_tokens.append('@usuario')\n",
    "                else:\n",
    "                    continue\n",
    "                \n",
    "                continue\n",
    "\n",
    "            if t.startswith('#'):\n",
    "                hashtag = t[1:]\n",
    "                t = hashtag\n",
    "\n",
    "            t = unicodedata.normalize('NFKC', t)\n",
    "            t = PUNCT_PATTERN.sub('', t)\n",
    "\n",
    "            if not t:\n",
    "                continue\n",
    "\n",
    "\n",
    "            if remove_stopwords and t in spanish_stopwords:\n",
    "                continue\n",
    "\n",
    "            if do_stemming:\n",
    "                try:\n",
    "                    t = stemmer.stem(t)\n",
    "                except Exception:\n",
    "                    pass\n",
    "\n",
    "            if not re.match(r'^[\\wáéíóúñüÁÉÍÓÚÑÜ0-9]+$', t):\n",
    "                continue\n",
    "\n",
    "            cleaned_tokens.append(t)\n",
    "\n",
    "        cleaned_tweets.append(' '.join(cleaned_tokens))\n",
    "    return cleaned_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "859533d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: @Pachifula Creo que hay algo de pasivoagresivo y manipulador.Gente que hace sentir culpable a otros (de cualquier vaina) es para aplicarles luego chantaje emocional.Un fren una vez me dijo que asumir que él está mal porque está desempleado es una \"microagresión\".#NoLoSoporto https://t.co/OVQD7OJzoZ\n",
      "\n",
      " Limpio:   @usuario creo que hay algo de pasivoagresivo y manipuladorgente que hace sentir culpable a otros de cualquier vaina es para aplicarles luego chantaje emocionalun fren una vez me dijo que asumir que él está mal porque está desempleado es una microagresión nolosoporto\n"
     ]
    }
   ],
   "source": [
    "X_train_clean = clean_tweets(X_train, remove_stopwords=False)\n",
    "X_val_clean   = clean_tweets(X_val, remove_stopwords=False)\n",
    "X_test_clean  = clean_tweets(X_test, remove_stopwords=False)\n",
    "\n",
    "print(\"Original:\", X_train[101])\n",
    "print(\"\\n Limpio:  \", X_train_clean[101])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c0aad23",
   "metadata": {},
   "source": [
    "Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b487814",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.DataFrame({ 'text': X_train_clean, 'label': y_train })\n",
    "df_val = pd.DataFrame({ 'text': X_val_clean, 'label': y_val })\n",
    "df_test = pd.DataFrame({'text': X_test_clean})\n",
    "\n",
    "df_train.to_csv('../data/train_clean.csv', index=False, encoding='utf-8')\n",
    "df_val.to_csv('../dat#a/val_clean.csv', index=False, encoding='utf-8')\n",
    "df_test.to_csv('../data/test_clean.csv', index=False, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
